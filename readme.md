# ä¸­æ–‡æƒ…æ„Ÿåˆ†ææ¨¡å‹ç¯å¢ƒé…ç½®ä¸ä½¿ç”¨æŒ‡å—

æœ¬é¡¹ç›®å±•ç¤ºå¦‚ä½•åœ¨æœ¬åœ°é…ç½®ç¯å¢ƒã€åŠ è½½é¢„è®­ç»ƒ BERT ä¸­æ–‡æ¨¡å‹ï¼Œå¹¶ä½¿ç”¨ HuggingFace ä¸Šçš„ ChnSentiCorp æ•°æ®é›†è¿›è¡Œä¸­æ–‡æƒ…æ„Ÿåˆ†æä»»åŠ¡ã€‚

## ğŸš€ 1. ç¯å¢ƒé…ç½®

### 1.1 åˆ›å»º Conda ç¯å¢ƒ

```bash
conda create -n hgf python=3.12 -y
conda activate hgf
```

### 1.2 å®‰è£… Transformersã€Datasetsã€torch

è‹¥éœ€è¦ GPU åŠ é€Ÿï¼Œè¯·æå‰å®‰è£…ä¸ä½  CUDA ç‰ˆæœ¬åŒ¹é…çš„ PyTorchã€‚

ğŸ”¹ å®‰è£… Transformersï¼ˆå« SentencePieceï¼‰

```bash
pip install transformers[sentencepiece]
```

ğŸ”¹ å®‰è£… Datasets

```bash
pip install datasets
```

ğŸ”¹å®‰è£… PyTorchï¼ˆGPU ç‰ˆæœ¬ï¼‰

è¯·æ ¹æ®ä½ çš„ CUDA ç‰ˆæœ¬é€‰æ‹©å®‰è£…å‘½ä»¤ï¼ˆç¤ºä¾‹ï¼šCUDA 12ï¼‰ï¼š

```bash
pip install torch --index-url https://download.pytorch.org/whl/cu121
```

## ğŸ¤– 2. åŠ è½½é¢„è®­ç»ƒ BERT ä¸­æ–‡æ¨¡å‹ä¸åˆ†è¯å™¨ï¼ˆtokenizerï¼‰

ä»¥ bert-base-chinese ä¸ºä¾‹ï¼šï¼ˆå¯ç”¨ä¸‹åˆ—è„šæœ¬åœ¨pythonä¸Šå®‰è£…ï¼Œä¹Ÿå¯å»Hugging Faceå®˜ç½‘æ‰¾åˆ°å¯¹åº”æ¨¡å‹ä¸‹è½½ï¼‰

 ```python
import transformers
from transformers import AutoModel,AutoTokenizer
model_name="bert-base-chinese"
model_dir=r"D:\cccc\usense_work\hgf_test\model"  ### æ¨¡å‹ä¿å­˜è·¯å¾„
model=AutoModel.from_pretrained(model_name,cache_dir=model_dir)
tokenizer=AutoTokenizer.from_pretrained(model_name,cache_dir=model_dir)
```

## ğŸ“¦ 3. åŠ è½½ä¸­æ–‡æƒ…æ„Ÿåˆ†ææ•°æ®é›†ï¼ˆChnSentiCorpï¼‰

HuggingFace æ•°æ®é›†ä»“åº“ï¼šhttps://huggingface.co/datasets/lansinuote/ChnSentiCorp

```python
from datasets import load_dataset
data_dir=r"D:\cccc\usense_work\hgf_test\data"
dataset = load_dataset("lansinuote/ChnSentiCorp",cache_dir=data_dir,num_proc=1)
dataset.save_to_disk(r"D:\cccc\usense_work\hgf_test\data\lansinuote___chn_senti_corp\data_csc")###æ•°æ®é›†ä¿å­˜è·¯å¾„
print(dataset["train"][0])
```


